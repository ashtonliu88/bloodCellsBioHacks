{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6R13f_N0XkM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.python.keras import optimizers\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.ops import state_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "import matplotlib.pylab as plt\n",
        "import librosa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration defaults\n",
        "DEFAULT_CONFIG = {\n",
        "    'general': {\n",
        "        'dataset_directory': 'data/audio_samples',\n",
        "    },\n",
        "    'preprocessing': {\n",
        "        'compute_embeddings': True,\n",
        "        'files_to_PCM': True,\n",
        "    },\n",
        "    'training': {\n",
        "        'epochs': 30,\n",
        "        'batch_size': 32,\n",
        "        'initial_lr': 0.001,\n",
        "        'use_validation': True,\n",
        "        'use_multiprocessing': True,\n",
        "        'mode': 'normal',\n",
        "        'ckpt_freq': 5\n",
        "    },\n",
        "    'model': {\n",
        "        'num_classes': None,\n",
        "        'input_shape': (96, 64, 1),  # Mel spectrogram dimensions\n",
        "    }\n",
        "}\n",
        "\n",
        "class Paths:\n",
        "    \"\"\"Helper class to manage paths\"\"\"\n",
        "    def __init__(self, base_dir='.'):\n",
        "        self.base_dir = base_dir\n",
        "        self.timestamp = None\n",
        "        self.CONF = None\n",
        "\n",
        "    def get_dataset_dir(self):\n",
        "        return os.path.join(self.base_dir, 'data', 'dataset')\n",
        "\n",
        "    def get_models_dir(self):\n",
        "        return os.path.join(self.base_dir, 'models')\n",
        "\n",
        "    def get_timestamped_dir(self):\n",
        "        return os.path.join(self.base_dir, 'runs', self.timestamp)\n",
        "\n",
        "    def get_checkpoints_dir(self):\n",
        "        return os.path.join(self.get_timestamped_dir(), 'checkpoints')\n",
        "\n",
        "    def get_stats_dir(self):\n",
        "        return os.path.join(self.get_timestamped_dir(), 'stats')\n",
        "\n",
        "    def get_embeddings_dir(self):\n",
        "        return os.path.join(self.get_timestamped_dir(), 'embeddings')\n",
        "\n",
        "    def get_ts_splits_dir(self):\n",
        "        return os.path.join(self.get_timestamped_dir(), 'splits')\n"
      ],
      "metadata": {
        "id": "ggCe_yIt0b6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class customAdam(optimizers.Adam):\n",
        "    \"\"\"Custom Adam optimizer with learning rate multiplier\"\"\"\n",
        "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0., amsgrad=False,\n",
        "                 lr_mult=0.1, excluded_vars=[], **kwargs):\n",
        "        super().__init__(lr=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, decay=decay, amsgrad=amsgrad, **kwargs)\n",
        "        with K.name_scope(self.__class__.__name__):\n",
        "            self.lr_mult = lr_mult\n",
        "            self.excluded_vars = excluded_vars\n",
        "\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [state_ops.assign_add(self.iterations, 1)]\n",
        "\n",
        "        lr = self.lr\n",
        "        if self.initial_decay > 0:\n",
        "            lr = lr * (1. / (1. + self.decay * math_ops.cast(self.iterations, K.dtype(self.decay))))\n",
        "\n",
        "        t = math_ops.cast(self.iterations, K.floatx()) + 1\n",
        "        lr_t = lr * (K.sqrt(1. - math_ops.pow(self.beta_2, t)) / (1. - math_ops.pow(self.beta_1, t)))\n",
        "\n",
        "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "\n",
        "        if self.amsgrad:\n",
        "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        else:\n",
        "            vhats = [K.zeros(1) for _ in params]\n",
        "\n",
        "        self.weights = [self.iterations] + ms + vs + vhats\n",
        "\n",
        "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
        "            multiplied_lr_t = lr_t * self.lr_mult if p.name not in self.excluded_vars else lr_t\n",
        "\n",
        "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
        "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * math_ops.square(g)\n",
        "\n",
        "            if self.amsgrad:\n",
        "                vhat_t = math_ops.maximum(vhat, v_t)\n",
        "                p_t = p - multiplied_lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n",
        "                self.updates.append(state_ops.assign(vhat, vhat_t))\n",
        "            else:\n",
        "                p_t = p - multiplied_lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
        "\n",
        "            self.updates.append(state_ops.assign(m, m_t))\n",
        "            self.updates.append(state_ops.assign(v, v_t))\n",
        "            new_p = p_t\n",
        "\n",
        "            if getattr(p, 'constraint', None) is not None:\n",
        "                new_p = p.constraint(new_p)\n",
        "\n",
        "            self.updates.append(state_ops.assign(p, new_p))\n",
        "        return self.updates\n"
      ],
      "metadata": {
        "id": "eaDjPHlpdhA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataSequence(tf.keras.utils.Sequence):\n",
        "    \"\"\"Data generator for training and validation\"\"\"\n",
        "    def __init__(self, X, y, batch_size, num_classes):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.X) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.X[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        # Load audio files and convert to spectrograms\n",
        "        X = np.array([self.load_audio(file_path) for file_path in batch_x])\n",
        "        y = tf.keras.utils.to_categorical(batch_y, num_classes=self.num_classes)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def load_audio(self, file_path):\n",
        "        if file_path.endswith('.npy'):\n",
        "            return np.load(file_path)\n",
        "        else:\n",
        "            # Load and preprocess audio file\n",
        "            audio, sr = librosa.load(file_path, sr=None)\n",
        "            mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
        "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "            return mel_spec_db\n"
      ],
      "metadata": {
        "id": "TsVk_m7vhqlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(conf, base_model=None):\n",
        "    \"\"\"Create or load the model architecture\"\"\"\n",
        "    if base_model is None:\n",
        "        # Create a simple CNN model if no base model is provided\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=conf['model']['input_shape']),\n",
        "            tf.keras.layers.MaxPooling2D(),\n",
        "            tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "            tf.keras.layers.MaxPooling2D(),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(conf['model']['num_classes'], activation='softmax')\n",
        "        ])\n",
        "        return model, model\n",
        "    else:\n",
        "        # Add classification head to base model\n",
        "        x = base_model.output\n",
        "        x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "        predictions = tf.keras.layers.Dense(conf['model']['num_classes'], activation='softmax')(x)\n",
        "        model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
        "        return model, base_model\n",
        "\n",
        "def train_model(conf=None):\n",
        "    \"\"\"Main training function\"\"\"\n",
        "    if conf is None:\n",
        "        conf = DEFAULT_CONFIG\n",
        "\n",
        "    # Setup paths\n",
        "    paths = Paths()\n",
        "    paths.timestamp = datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
        "\n",
        "    # Create necessary directories\n",
        "    os.makedirs(paths.get_timestamped_dir(), exist_ok=True)\n",
        "    os.makedirs(paths.get_checkpoints_dir(), exist_ok=True)\n",
        "    os.makedirs(paths.get_stats_dir(), exist_ok=True)\n",
        "\n",
        "    # Load and prepare data\n",
        "    train_files = os.listdir(os.path.join(paths.get_dataset_dir(), 'train'))\n",
        "    train_files = [os.path.join(paths.get_dataset_dir(), 'train', f) for f in train_files]\n",
        "    train_labels = [0] * len(train_files)  # Replace with actual labels\n",
        "\n",
        "    if conf['training']['use_validation']:\n",
        "        val_files = os.listdir(os.path.join(paths.get_dataset_dir(), 'val'))\n",
        "        val_files = [os.path.join(paths.get_dataset_dir(), 'val', f) for f in val_files]\n",
        "        val_labels = [0] * len(val_files)  # Replace with actual labels\n",
        "\n",
        "    # Create data generators\n",
        "    train_gen = DataSequence(train_files, train_labels,\n",
        "                            conf['training']['batch_size'],\n",
        "                            conf['model']['num_classes'])\n",
        "\n",
        "    val_gen = None\n",
        "    if conf['training']['use_validation']:\n",
        "        val_gen = DataSequence(val_files, val_labels,\n",
        "                              conf['training']['batch_size'],\n",
        "                              conf['model']['num_classes'])\n",
        "\n",
        "    # Create and compile model\n",
        "    model, base_model = create_model(conf)\n",
        "\n",
        "    # Get top layer variables\n",
        "    base_vars = [var.name for var in base_model.trainable_variables]\n",
        "    all_vars = [var.name for var in model.trainable_variables]\n",
        "    top_vars = list(set(all_vars) - set(base_vars))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=customAdam(\n",
        "            lr=conf['training']['initial_lr'],\n",
        "            amsgrad=True,\n",
        "            lr_mult=0.1,\n",
        "            excluded_vars=top_vars\n",
        "        ),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        epochs=conf['training']['epochs'],\n",
        "        validation_data=val_gen,\n",
        "        use_multiprocessing=conf['training']['use_multiprocessing'],\n",
        "        workers=4\n",
        "    )\n",
        "\n",
        "    # Save model and training stats\n",
        "    model.save(os.path.join(paths.get_checkpoints_dir(), 'final_model.h5'))\n",
        "\n",
        "    stats = {\n",
        "        'epoch': list(range(1, len(history.history['loss']) + 1)),\n",
        "        'training_time': time.time(),\n",
        "        'timestamp': paths.timestamp\n",
        "    }\n",
        "    stats.update(history.history)\n",
        "\n",
        "    with open(os.path.join(paths.get_stats_dir(), 'stats.json'), 'w') as f:\n",
        "        json.dump(stats, f, indent=4)\n",
        "\n",
        "    return model, history\n"
      ],
      "metadata": {
        "id": "jn5AXtFkiQwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Set up GPU memory growth\n",
        "    gpu_options = tf.GPUOptions(allow_growth=True)\n",
        "    tfconfig = tf.ConfigProto(gpu_options=gpu_options)\n",
        "    sess = tf.Session(config=tfconfig)\n",
        "    K.set_session(sess)\n",
        "\n",
        "    # Train the model\n",
        "    model, history = train_model()\n",
        "\n",
        "    # Plot training results\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    if 'val_loss' in history.history:\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    if 'val_accuracy' in history.history:\n",
        "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jopWerQKiZW_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}