{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries and data"
      ],
      "metadata": {
        "id": "cFXtKccYpqI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "TRAINING_ONLY = True"
      ],
      "metadata": {
        "id": "cOOYT_Vdh3H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rFvmC_TN6uq"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"vbookshelf/respiratory-sound-database\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/root/.cache/kagglehub/datasets/vbookshelf/respiratory-sound-database/versions/2'\n",
        "\n",
        "audio_path = os.path.join(dataset_path, 'respiratory_sound_database','Respiratory_Sound_Database', 'audio_and_txt_files')\n",
        "audio_files = glob.glob(os.path.join(audio_path, '**/*.wav'), recursive=True)\n",
        "\n",
        "# print(glob.glob(os.path.join(dataset_path, '*'), recursive=True))\n",
        "\n",
        "print(f\"Found {len(audio_files)} audio files.\")"
      ],
      "metadata": {
        "id": "NWY4eMRZhFKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "pm7xRNYGPula"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "_xhx_zdhS3c3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "5IGeLxDBlxxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patient_diagnosis = os.path.join(dataset_path, 'demographic_info.txt')\n",
        "col_names = ['patient_id', 'age', 'sex', 'adult_bmi', 'child_weight', 'child_height']\n",
        "df_demo = pd.read_csv(patient_diagnosis, sep=\" \", header=None, names=col_names)\n",
        "\n",
        "\n",
        "# Load the disease diagnosis information\n",
        "diagnosis_mapping = {}\n",
        "diagnosis_file = os.path.join(dataset_path, 'demographic_info.txt')\n",
        "with open(diagnosis_file, 'r') as f:\n",
        "    for line in f:\n",
        "        parts_diag = line.strip().split()\n",
        "        if len(parts_diag) >= 2:\n",
        "            patient_id = parts_diag[0]\n",
        "            diagnosis = parts_diag[1]\n",
        "            diagnosis_mapping[patient_id] = diagnosis"
      ],
      "metadata": {
        "id": "SvMkorp_1Ka9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patient_diagnosis = os.path.join(dataset_path, 'respiratory_sound_database','Respiratory_Sound_Database', 'patient_diagnosis.csv')\n",
        "df_diag = pd.read_csv(patient_diagnosis, header=None, names=['patient_id', 'diagnosis'])\n",
        "print(df_diag['diagnosis'].value_counts())"
      ],
      "metadata": {
        "id": "jeC0T54n1M3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(file_path, sr=22050, n_mfcc=13):\n",
        "    \"\"\"\n",
        "    Extract audio features including MFCCs, spectral centroid,\n",
        "    zero crossing rate, spectral bandwidth, and chroma features.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(file_path, sr=sr)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "    if y.size == 0:\n",
        "        print(f\"File {file_path} is empty.\")\n",
        "        return None\n",
        "\n",
        "    # MFCCs\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "    mfcc_mean = np.mean(mfcc, axis=1)\n",
        "    mfcc_std = np.std(mfcc, axis=1)\n",
        "\n",
        "    # Spectral centroid\n",
        "    spec_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "    spec_centroid_mean = np.mean(spec_centroid)\n",
        "    spec_centroid_std = np.std(spec_centroid)\n",
        "\n",
        "    # Zero Crossing Rate\n",
        "    zcr = librosa.feature.zero_crossing_rate(y)\n",
        "    zcr_mean = np.mean(zcr)\n",
        "    zcr_std = np.std(zcr)\n",
        "\n",
        "    # Spectral bandwidth\n",
        "    spec_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "    spec_bandwidth_mean = np.mean(spec_bandwidth)\n",
        "    spec_bandwidth_std = np.std(spec_bandwidth)\n",
        "\n",
        "    # Chroma features\n",
        "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    chroma_mean = np.mean(chroma, axis=1)\n",
        "    chroma_std = np.std(chroma, axis=1)\n",
        "\n",
        "    # Combine features into a dictionary\n",
        "    features = {}\n",
        "    for i in range(n_mfcc):\n",
        "        features[f'mfcc_{i+1}_mean'] = mfcc_mean[i]\n",
        "        features[f'mfcc_{i+1}_std'] = mfcc_std[i]\n",
        "\n",
        "    features['spec_centroid_mean'] = spec_centroid_mean\n",
        "    features['spec_centroid_std'] = spec_centroid_std\n",
        "    features['zcr_mean'] = zcr_mean\n",
        "    features['zcr_std'] = zcr_std\n",
        "    features['spec_bandwidth_mean'] = spec_bandwidth_mean\n",
        "    features['spec_bandwidth_std'] = spec_bandwidth_std\n",
        "\n",
        "    for i in range(chroma.shape[0]):\n",
        "        features[f'chroma_{i+1}_mean'] = chroma_mean[i]\n",
        "        features[f'chroma_{i+1}_std'] = chroma_std[i]\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "MHy3Pt81hd-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Augmentation functions (no redefinition of time_stretch)\n",
        "def pitch_shift(y, sr, n_steps=4):\n",
        "    \"\"\"Pitch shift the audio signal by n_steps semitones.\"\"\"\n",
        "    return librosa.effects.pitch_shift(y, sr=sr, n_steps=n_steps)  # Pass `sr` as a keyword argument\n",
        "\n",
        "def add_noise(y, noise_level=0.005):\n",
        "    \"\"\"Inject random noise into the audio signal.\"\"\"\n",
        "    noise = np.random.randn(len(y)) * noise_level\n",
        "    return y + noise\n",
        "\n",
        "def change_volume(y, gain=1.5):\n",
        "    \"\"\"Increase or decrease the volume by a certain gain factor.\"\"\"\n",
        "    return y * gain\n",
        "\n",
        "def apply_random_augmentation(y, sr):\n",
        "    \"\"\"Randomly apply one of the augmentations.\"\"\"\n",
        "    augmentation_type = random.choice([\"time_stretch\", \"pitch_shift\", \"add_noise\", \"change_volume\", None])\n",
        "\n",
        "    if augmentation_type == \"time_stretch\":\n",
        "        # Directly use librosa.effects.time_stretch here\n",
        "        rate = random.uniform(0.8, 1.5)  # Random rate between 0.8 and 1.5\n",
        "        y = librosa.effects.time_stretch(y, rate=rate)\n",
        "    elif augmentation_type == \"pitch_shift\":\n",
        "        y = pitch_shift(y, sr, n_steps=random.randint(-5, 5))  # Random pitch shift between -5 and 5 semitones\n",
        "    elif augmentation_type == \"add_noise\":\n",
        "        y = add_noise(y, noise_level=random.uniform(0.001, 0.01))  # Random noise level\n",
        "    elif augmentation_type == \"change_volume\":\n",
        "        y = change_volume(y, gain=random.uniform(0.5, 2.0))  # Random gain between 0.5 and 2.0\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "7m0RywB397l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_list = []\n",
        "labels = []\n",
        "\n",
        "for file in audio_files:\n",
        "    feats = extract_features(file)\n",
        "    if feats is None:\n",
        "        continue\n",
        "    feature_list.append(feats)\n",
        "\n",
        "    # Extract patient ID from the file name (first part)\n",
        "    file_name = os.path.basename(file)\n",
        "    parts = file_name.split('_')\n",
        "    if len(parts) >= 1:\n",
        "        patient_id = parts[0]\n",
        "        diagnosis_row = df_diag[df_diag['patient_id'] == int(patient_id)]\n",
        "\n",
        "        if not diagnosis_row.empty:\n",
        "            diagnosis = diagnosis_row['diagnosis'].values[0]\n",
        "\n",
        "            if diagnosis == \"Healthy\":\n",
        "                label = \"healthy\"\n",
        "            elif diagnosis == 'COPD':\n",
        "              label = 'COPD'\n",
        "            elif diagnosis == 'LRTI':\n",
        "              label = 'LRTI'\n",
        "            elif diagnosis == 'URTI':\n",
        "              label = 'URTI'\n",
        "            elif diagnosis == 'Bronchiectasis':\n",
        "              label = 'Bronchiectasis'\n",
        "            elif diagnosis == 'Pneumonia':\n",
        "              label = 'Pneumonia'\n",
        "            elif diagnosis == 'Bronchiolitis':\n",
        "              label = 'Bronchiolitis'\n",
        "            else:\n",
        "                label = \"unknown\"\n",
        "        else:\n",
        "            label = \"unknown\"\n",
        "            print(f\"Warning: Missing diagnosis for patient {patient_id}. Label set to 'unknown'.\")\n",
        "\n",
        "    else:\n",
        "        label = \"unknown\"\n",
        "\n",
        "    labels.append(label)"
      ],
      "metadata": {
        "id": "C_R5SzDtvWfC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame from features and labels\n",
        "df_features = pd.DataFrame(feature_list)\n",
        "df_features['label'] = labels\n",
        "\n",
        "df_features = df_features[df_features['label'] != 'unknown']\n",
        "\n",
        "# Debug: Check label distribution before filtering\n",
        "print(\"Feature DataFrame shape:\", df_features.shape)\n",
        "print(\"-\"*50)\n",
        "print(df_features['label'].value_counts())\n",
        "# print(\"-\"*50)\n",
        "# print(df_features.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4saZheT1oWO7",
        "outputId": "614a719d-a5ba-4e11-d4d1-7ada7ebb03dd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature DataFrame shape: (919, 57)\n",
            "--------------------------------------------------\n",
            "label\n",
            "COPD              793\n",
            "Pneumonia          37\n",
            "healthy            35\n",
            "URTI               23\n",
            "Bronchiectasis     16\n",
            "Bronchiolitis      13\n",
            "LRTI                2\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame from augmented features and labels\n",
        "df_augmented_features = pd.DataFrame(augmented_feature_list)\n",
        "df_augmented_features['label'] = augmented_labels\n",
        "\n",
        "# Check the augmented feature DataFrame shape\n",
        "print(\"Augmented Feature DataFrame shape:\", df_augmented_features.shape)\n",
        "print(\"-\" * 50)\n",
        "print(df_augmented_features['label'].value_counts())\n",
        "\n",
        "# Combine augmented data with the original data\n",
        "df_combined = pd.concat([df_features, df_augmented_features], ignore_index=True)\n",
        "\n",
        "# Shuffle the combined dataset\n",
        "df_combined = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split into train-test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_combined.drop('label', axis=1),\n",
        "                                                    df_combined['label'],\n",
        "                                                    test_size=0.2, random_state=42)\n",
        "\n",
        "# Check new label distribution after augmentation\n",
        "print(\"Combined label distribution after augmentation:\")\n",
        "print(df_combined['label'].value_counts())"
      ],
      "metadata": {
        "id": "BipjdqRm_7wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building"
      ],
      "metadata": {
        "id": "dqQ2ROAgpfd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RandomForest Classification"
      ],
      "metadata": {
        "id": "xTOJdScdr0Sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "50bPizIPtuI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Random Forest classifier\n",
        "# clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf = RandomForestClassifier(\n",
        "    n_estimators=50,              # Reduce the number of trees\n",
        "    max_depth=10,                 # Limit the depth of trees to prevent overfitting\n",
        "    min_samples_split=6,          # Require more samples to split a node\n",
        "    min_samples_leaf=4,           # Require more samples at the leaf node\n",
        "    max_features=0.5,             # Consider only 50% of features at each split\n",
        "    bootstrap=True,               # Use bootstrapped samples for each tree\n",
        "    random_state=42\n",
        ")\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "SQm_FximBdJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions and evaluate the classifier\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "3SpC6v8pFYXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Classification Report\n",
        "cr = classification_report(y_test, y_pred, target_names=le.classes_)\n",
        "print(\"Classification Report:\")\n",
        "print(cr)\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "cv_scores = cross_val_score(rf, X, y, cv=5, scoring='accuracy')\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean CV score:\", cv_scores.mean())\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [5, 10, None],\n",
        "    'min_samples_split': [2, 4, 8],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")"
      ],
      "metadata": {
        "id": "Gee7AwbA7ils"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "_nRSoWExCBdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "gfHnvh6TDIb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline that first scales the data then trains an SVM\n",
        "svm_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    # ('svc', SVC(kernel='rbf', probability=True, random_state=42))\n",
        "    ('svc', SVC(\n",
        "        kernel='linear',\n",
        "        probability=True,\n",
        "        C=0.1,\n",
        "        random_state=42\n",
        "        ))\n",
        "])\n",
        "\n",
        "# Define a grid of hyperparameters to search over\n",
        "param_grid = {\n",
        "    # 'svc__C': [0.1, 1, 10, 100],\n",
        "    'svc__C': [0.1, 1, 10]\n",
        "    # 'svc__gamma': [0.001, 0.01, 0.1, 1],\n",
        "}\n",
        "\n",
        "# Set up grid search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(svm_pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "print(\"Best cross validation accuracy:\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "drYXAEJYCbar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set and evaluate accuracy\n",
        "y_pred_svm = grid_search.predict(X_test)\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "svm = grid_search.best_estimator_\n",
        "print(f\"SVM Test Accuracy: {accuracy_svm*100:.2f}%\")"
      ],
      "metadata": {
        "id": "TMRMpjN3ER6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_svm)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Classification Report\n",
        "cr = classification_report(y_test, y_pred_svm, target_names=le.classes_)\n",
        "print(\"Classification Report:\")\n",
        "print(cr)\n",
        "\n",
        "# Evaluate the best SVM pipeline using cross-validation on the training set\n",
        "cv_scores_svm = cross_val_score(grid_search.best_estimator_, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(\"SVM Cross-validation scores:\", cv_scores_svm)\n",
        "print(\"Mean SVM CV score:\", cv_scores_svm.mean())"
      ],
      "metadata": {
        "id": "WblvEqXK-J7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble"
      ],
      "metadata": {
        "id": "iNpmlkLIFNfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "base_learners = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)),\n",
        "    ('svm', SVC(kernel='linear', C=0.1, random_state=42))\n",
        "]\n",
        "\n",
        "stack_model = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\n",
        "stack_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_stack = stack_model.predict(X_test)\n",
        "accuracy_stack = accuracy_score(y_test, y_pred_stack)\n",
        "print(f\"Stacking Model Test Accuracy: {accuracy_stack*100:.2f}%\")"
      ],
      "metadata": {
        "id": "v_yado477ZkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# # Create an ensemble classifier with soft voting\n",
        "# ensemble_model = VotingClassifier(\n",
        "#     estimators=[('rf', clf), ('svm', svm)],\n",
        "#     voting='soft',\n",
        "#     weights=[3, 2]\n",
        "# )\n",
        "\n",
        "# # Train the ensemble model on the training data\n",
        "# ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "# # Predict on the test set and evaluate accuracy\n",
        "# y_pred_ensemble = ensemble_model.predict(X_test)\n",
        "# accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
        "# print(f\"Ensemble Model Test Accuracy: {accuracy_ensemble*100:.2f}%\")"
      ],
      "metadata": {
        "id": "8CJGyImsFOuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try out"
      ],
      "metadata": {
        "id": "4eMXNNm4GoXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_lung_cancer_probability(wav_file, ensemble_model, feature_columns, label_encoder):\n",
        "    # Extract features using the same function as for training\n",
        "    feats = extract_features(wav_file)\n",
        "    if feats is None:\n",
        "        print(f\"Error extracting features from {wav_file}.\")\n",
        "        return None\n",
        "\n",
        "    input_data = pd.DataFrame([feats])\n",
        "    input_data = input_data.reindex(columns=feature_columns, fill_value=0)\n",
        "\n",
        "    # Predict probabilities using the ensemble model\n",
        "    probas = ensemble_model.predict_proba(input_data)\n",
        "    diseased_index = np.where(label_encoder.classes_ == 'diseased')[0][0]\n",
        "    probability_diseased = probas[0, diseased_index]\n",
        "    return probability_diseased"
      ],
      "metadata": {
        "id": "jHbPGPLCGpQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wav_tests = glob.glob('/content/test/*.wav')\n",
        "feature_columns = df_features.drop(\"label\", axis=1).columns"
      ],
      "metadata": {
        "id": "8uNXviKgGvlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for test in wav_tests:\n",
        "    probability = predict_lung_cancer_probability(test, ensemble_model, feature_columns, le)\n",
        "    if probability is not None:\n",
        "        print(f\"Probability of lung cancer: {probability*100:.2f}%\")"
      ],
      "metadata": {
        "id": "RmmpPk1SdUAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# Wrap your ensemble classifier with calibration\n",
        "calibrated_ensemble = CalibratedClassifierCV(ensemble_model, cv=5, method='isotonic')\n",
        "calibrated_ensemble.fit(X_train, y_train)\n",
        "\n",
        "# Now predict probabilities on new samples\n",
        "for test in wav_tests:\n",
        "    probability = predict_lung_cancer_probability(test, calibrated_ensemble, feature_columns, le)\n",
        "    if probability is not None:\n",
        "        print(f\"Calibrated probability of lung cancer: {probability*100:.2f}%\")"
      ],
      "metadata": {
        "id": "HswO7ldPbTgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rtl7x5_XdbEA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}